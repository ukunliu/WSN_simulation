{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "cirs = np.load('cirs_noised_ld.npy')\n",
    "cirs_obs = np.load('cirs_observation_ld.npy')\n",
    "triplets = np.load('triplets_ld.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "miu, sigma = cirs.mean(), cirs.var()\n",
    "\n",
    "cirs = (cirs - cirs.min()) / (cirs.max() - cirs.min())\n",
    "cirs_obs = (cirs_obs - cirs_obs.min()) / (cirs_obs.max() - cirs_obs.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = np.array([cirs_obs[trp[0]] for trp in triplets])\n",
    "Pi = np.array([cirs[trp[1]] for trp in triplets])\n",
    "Pj = np.array([cirs[trp[2]] for trp in triplets])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_hinge(p, pi, pj, W, margin=1):\n",
    "    return max(0, margin - p.T @ W @ pi + p.T @ W @ pj)\n",
    "\n",
    "def loss_global(triplets, W, cirs, cirs_obs):\n",
    "    return np.sum([loss_hinge(cirs_obs[i0], cirs[i1], cirs[i2], W) for i0, i1, i2 in triplets])\n",
    "\n",
    "def gradient(W, P, Pi, Pj):\n",
    "    grad = np.zeros_like(W)\n",
    "\n",
    "def gradient_descent():\n",
    "    ..."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online Algorithm for Scalable Image Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = 1000\n",
    "margin = 1e4\n",
    "n, d = cirs.shape # number of samples, dimension of samples\n",
    "theta = np.ones((d, d)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = [loss_0]\n",
    "\n",
    "for ind_0, ind_1, ind_2 in t:\n",
    "    p, pi, pj = c_o[ind_0], c[ind_1], c[ind_2]\n",
    "    v = p * (pi - pj)\n",
    "    tau = min(C, loss_hinge(p, pi, pj, theta, margin) / (v.T @ v))\n",
    "    theta = theta + tau * v\n",
    "    loss.append(loss_global(triplets, theta, cirs, cirs_obs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_hinge(p, pi, pj, theta, margin) / (v.T @ v)\n",
    "loss_hinge(p, pi, pj, theta, margin)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Triplet loss with Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dense\n",
    "from tensorflow.keras.layers import Lambda, Dot\n",
    "from tensorflow.keras.regularizers import l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def identity_loss(y_true, y_pred):\n",
    "    \"\"\"Ignore y_true and return the mean of y_pred\n",
    "    \n",
    "    This is a hack to work-around the design of the Keras API that is\n",
    "    not really suited to train networks with a triplet loss by default.\n",
    "    \"\"\"\n",
    "    return tf.reduce_mean(y_pred)\n",
    "\n",
    "\n",
    "class MarginLoss(layers.Layer):\n",
    "\n",
    "    def __init__(self, margin=1.):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        pos_pair_similarity = inputs[0]\n",
    "        neg_pair_similarity = inputs[1]\n",
    "        \n",
    "        diff = neg_pair_similarity - pos_pair_similarity\n",
    "        return tf.maximum(diff + self.margin, 0.)\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Input, Dense\n",
    "from tensorflow.keras.layers import Lambda, Dot\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "\n",
    "class TripletModel(Model):\n",
    "    def __init__(self, n_users, n_items, latent_dim=64,\n",
    "                 l2_reg=None, margin=1.):\n",
    "        super().__init__(name=\"TripletModel\")\n",
    "        \n",
    "        self.margin = margin\n",
    "        \n",
    "        l2_reg = None if l2_reg == 0 else l2(l2_reg)\n",
    "\n",
    "        self.user_layer = Embedding(n_users, latent_dim,\n",
    "                                    input_length=1,\n",
    "                                    input_shape=(1,),\n",
    "                                    name='user_embedding',\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "    \n",
    "        # The following embedding parameters will be shared to\n",
    "        # encode both the positive and negative items.\n",
    "        self.item_layer = Embedding(n_items, latent_dim,\n",
    "                                    input_length=1,\n",
    "                                    name=\"item_embedding\",\n",
    "                                    embeddings_regularizer=l2_reg)\n",
    "        \n",
    "        # The 2 following layers are without parameters, and can\n",
    "        # therefore be used for both positive and negative items.\n",
    "        self.flatten = Flatten()\n",
    "        self.dot = Dot(axes=1, normalize=True)\n",
    "\n",
    "        self.margin_loss = MarginLoss(margin)\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        user_input = inputs[0]\n",
    "        pos_item_input = inputs[1]\n",
    "        neg_item_input = inputs[2]\n",
    "        \n",
    "        user_embedding = self.user_layer(user_input)\n",
    "        user_embedding = self.flatten(user_embedding)\n",
    "        \n",
    "        pos_item_embedding = self.item_layer(pos_item_input)\n",
    "        pos_item_embedding = self.flatten(pos_item_embedding)\n",
    "        \n",
    "        neg_item_embedding = self.item_layer(neg_item_input)\n",
    "        neg_item_embedding = self.flatten(neg_item_embedding)\n",
    "        \n",
    "        # Similarity computation between embeddings\n",
    "        pos_similarity = self.dot([user_embedding, pos_item_embedding])\n",
    "        neg_similarity = self.dot([user_embedding, neg_item_embedding])\n",
    "                \n",
    "        return self.margin_loss([pos_similarity, neg_similarity])\n",
    "    \n",
    "\n",
    "def average_roc_auc(model, data_train, data_test):\n",
    "    \"\"\"Compute the ROC AUC for each user and average over users\"\"\"\n",
    "    max_user_id = max(data_train['user_id'].max(),\n",
    "                      data_test['user_id'].max())\n",
    "    max_item_id = max(data_train['item_id'].max(),\n",
    "                      data_test['item_id'].max())\n",
    "    user_auc_scores = []\n",
    "    for user_id in range(1, max_user_id + 1):\n",
    "        pos_item_train = data_train[data_train['user_id'] == user_id]\n",
    "        pos_item_test = data_test[data_test['user_id'] == user_id]\n",
    "        \n",
    "        # Consider all the items already seen in the training set\n",
    "        all_item_ids = np.arange(1, max_item_id + 1)\n",
    "        items_to_rank = np.setdiff1d(\n",
    "            all_item_ids, pos_item_train['item_id'].values)\n",
    "        \n",
    "        # Ground truth: return 1 for each item positively present in\n",
    "        # the test set and 0 otherwise.\n",
    "        expected = np.in1d(\n",
    "            items_to_rank, pos_item_test['item_id'].values)\n",
    "        \n",
    "        if np.sum(expected) >= 1:\n",
    "            # At least one positive test value to rank\n",
    "            repeated_user_id = np.empty_like(items_to_rank)\n",
    "            repeated_user_id.fill(user_id)\n",
    "\n",
    "            predicted = model.predict(\n",
    "                [repeated_user_id, items_to_rank], batch_size=4096)\n",
    "        \n",
    "            user_auc_scores.append(roc_auc_score(expected, predicted))\n",
    "\n",
    "    return sum(user_auc_scores) / len(user_auc_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_triplets():\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "triplet_model = TripletModel(n_users, n_items,\n",
    "                             latent_dim=64, l2_reg=1e-6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (computer_vision)",
   "language": "python",
   "name": "pycharm-dac32f44"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
